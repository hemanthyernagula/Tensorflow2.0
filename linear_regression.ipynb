{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (w*x + b)\n",
    "<strong> Implementing Linear Regression with Tensorflow2.0</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA\n",
       "0  1714  2.40\n",
       "1  1664  2.52\n",
       "2  1760  2.54\n",
       "3  1685  2.74\n",
       "4  1693  2.83"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('linear_regression.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = data.SAT\n",
    "ytrain = data.GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(np.random.randn(),name='weight')\n",
    "b = tf.Variable(np.random.randn(),name='intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(x):\n",
    "    \n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfun(actual,pred):\n",
    "    return tf.reduce_sum(tf.pow(pred - actual,2)) / (xtrain.shape[0])\n",
    "\n",
    "optimizer = tf.optimizers.Adam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize():\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_regression(xtrain)\n",
    "        loss = lossfun(ytrain,pred)\n",
    "    grad = g.gradient(loss,[w,b])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grad,[w,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir logs/training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 5064340.0\n",
      "Epoch 50 loss 1910387.75\n",
      "Epoch 100 loss 537098.25\n",
      "Epoch 150 loss 108765.4765625\n",
      "Epoch 200 loss 15315.23828125\n",
      "Epoch 250 loss 1464.7484130859375\n",
      "Epoch 300 loss 92.64915466308594\n",
      "Epoch 350 loss 3.743489980697632\n",
      "Epoch 400 loss 0.1302185356616974\n",
      "Epoch 450 loss 0.04515611380338669\n",
      "Epoch 500 loss 0.04417206719517708\n",
      "Epoch 550 loss 0.04416856914758682\n",
      "Epoch 600 loss 0.04416856914758682\n",
      "Epoch 650 loss 0.044168561697006226\n",
      "Epoch 700 loss 0.04416855797171593\n",
      "Epoch 750 loss 0.04416855052113533\n",
      "Epoch 800 loss 0.04416854679584503\n",
      "Epoch 850 loss 0.04416854307055473\n",
      "Epoch 900 loss 0.044168539345264435\n",
      "Epoch 950 loss 0.04416852071881294\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    \n",
    "    logs = tf.summary.create_file_writer('/logs/training/')\n",
    "    with logs.as_default():\n",
    "        optimize()\n",
    "        if i % 50 == 0:\n",
    "            ypred = linear_regression(xtrain)\n",
    "            loss  = lossfun(ytrain,ypred)\n",
    "            print('Epoch {} loss {}'.format(i,loss))\n",
    "            tf.summary.scalar('Loss',loss,step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : 1714 Y : 2.40 Y^ : 3.074080467224121\n",
      "X : 1664 Y : 2.52 Y^ : 2.977144241333008\n",
      "X : 1760 Y : 2.54 Y^ : 3.163261890411377\n",
      "X : 1685 Y : 2.74 Y^ : 3.017857313156128\n",
      "X : 1693 Y : 2.83 Y^ : 3.033367156982422\n",
      "X : 1670 Y : 2.91 Y^ : 2.988776445388794\n",
      "X : 1764 Y : 3.00 Y^ : 3.1710166931152344\n",
      "X : 1764 Y : 3.00 Y^ : 3.1710166931152344\n",
      "X : 1792 Y : 3.01 Y^ : 3.2253010272979736\n",
      "X : 1850 Y : 3.01 Y^ : 3.337747097015381\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('X : {} Y : {:.2f} Y^ : {}'.format(xtrain[i],ytrain[i],linear_regression(xtrain[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xtrain,ytrain,'ro',label='Data')\n",
    "plt.plot(xtrain,w*xtrain +b,label='Best Line')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self,callback=False,\n",
    "                 optimizer = tf.optimizers.Adam(0.01),\n",
    "                 epochs = 10,verbose=0):\n",
    "        \n",
    "        self.w = tf.Variable(np.random.randn(),name='Weights')\n",
    "        self.b = tf.Variable(np.random.randn(),name='Intercept')\n",
    "        self.callback  =  callback\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs    = epochs\n",
    "        self.verbose   = verbose\n",
    "    def linear_regression(self,x):\n",
    "        return self.w * x + self.b\n",
    "    def loss_fun(self,actual,pred):\n",
    "        return tf.reduce_sum(tf.pow(pred - actual,2)) / actual.shape[0]\n",
    "    def optimize(self,x,y):\n",
    "        \n",
    "        with tf.GradientTape() as g:\n",
    "            pred = self.linear_regression(x)\n",
    "            loss = self.loss_fun(y,pred)\n",
    "            \n",
    "        grad = g.gradient(loss,[self.w,self.b])\n",
    "        self.optimizer.apply_gradients(zip(grad,[self.w,self.b]))\n",
    "        \n",
    "    def fit(self,x,y):\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            self.optimize(x,y)\n",
    "            \n",
    "            space = len(str(self.epochs))\n",
    "            if self.verbose:\n",
    "                if i % self.verbose :\n",
    "                    pred = self.linear_regression(x)\n",
    "                    loss = self.loss_fun(y,pred)\n",
    "                    print('Epoch :',' '*(space - len(str(i))),end=\" \")\n",
    "                    print('{} | Loss : {}'.format(i,loss))\n",
    "                    \n",
    "    def predict(self,x):\n",
    "        return self.w * x + self.b\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (29,)\n",
      "Shape of test: (8,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(xtrain,ytrain,test_size=0.2)\n",
    "\n",
    "\n",
    "print('Shape of train:',xtrain.shape)\n",
    "print('Shape of test:',xtest.shape)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = tf.optimizers.Adam(0.01)\n",
    "linear = LinearRegression(verbose=10,epochs=1000,optimizer=optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :     1 | Loss : 8051557.5\n",
      "Epoch :     2 | Loss : 7946098.0\n",
      "Epoch :     3 | Loss : 7841376.5\n",
      "Epoch :     4 | Loss : 7737404.5\n",
      "Epoch :     5 | Loss : 7634195.0\n",
      "Epoch :     6 | Loss : 7531754.0\n",
      "Epoch :     7 | Loss : 7430096.5\n",
      "Epoch :     8 | Loss : 7329231.5\n",
      "Epoch :     9 | Loss : 7229166.5\n",
      "Epoch :    11 | Loss : 7031476.5\n",
      "Epoch :    12 | Loss : 6933869.0\n",
      "Epoch :    13 | Loss : 6837094.5\n",
      "Epoch :    14 | Loss : 6741163.0\n",
      "Epoch :    15 | Loss : 6646079.0\n",
      "Epoch :    16 | Loss : 6551851.0\n",
      "Epoch :    17 | Loss : 6458483.5\n",
      "Epoch :    18 | Loss : 6365980.5\n",
      "Epoch :    19 | Loss : 6274348.0\n",
      "Epoch :    21 | Loss : 6093707.5\n",
      "Epoch :    22 | Loss : 6004707.5\n",
      "Epoch :    23 | Loss : 5916591.5\n",
      "Epoch :    24 | Loss : 5829359.5\n",
      "Epoch :    25 | Loss : 5743017.0\n",
      "Epoch :    26 | Loss : 5657561.5\n",
      "Epoch :    27 | Loss : 5572996.5\n",
      "Epoch :    28 | Loss : 5489321.0\n",
      "Epoch :    29 | Loss : 5406534.5\n",
      "Epoch :    31 | Loss : 5243629.0\n",
      "Epoch :    32 | Loss : 5163508.5\n",
      "Epoch :    33 | Loss : 5084273.5\n",
      "Epoch :    34 | Loss : 5005921.5\n",
      "Epoch :    35 | Loss : 4928452.5\n",
      "Epoch :    36 | Loss : 4851863.0\n",
      "Epoch :    37 | Loss : 4776150.0\n",
      "Epoch :    38 | Loss : 4701311.5\n",
      "Epoch :    39 | Loss : 4627342.5\n",
      "Epoch :    41 | Loss : 4482001.5\n",
      "Epoch :    42 | Loss : 4410623.0\n",
      "Epoch :    43 | Loss : 4340098.5\n",
      "Epoch :    44 | Loss : 4270425.0\n",
      "Epoch :    45 | Loss : 4201598.5\n",
      "Epoch :    46 | Loss : 4133613.75\n",
      "Epoch :    47 | Loss : 4066465.0\n",
      "Epoch :    48 | Loss : 4000148.5\n",
      "Epoch :    49 | Loss : 3934658.5\n",
      "Epoch :    51 | Loss : 3806137.75\n",
      "Epoch :    52 | Loss : 3743095.5\n",
      "Epoch :    53 | Loss : 3680859.25\n",
      "Epoch :    54 | Loss : 3619422.25\n",
      "Epoch :    55 | Loss : 3558779.5\n",
      "Epoch :    56 | Loss : 3498925.25\n",
      "Epoch :    57 | Loss : 3439851.75\n",
      "Epoch :    58 | Loss : 3381555.75\n",
      "Epoch :    59 | Loss : 3324029.75\n",
      "Epoch :    61 | Loss : 3211264.75\n",
      "Epoch :    62 | Loss : 3156013.25\n",
      "Epoch :    63 | Loss : 3101507.75\n",
      "Epoch :    64 | Loss : 3047742.25\n",
      "Epoch :    65 | Loss : 2994710.5\n",
      "Epoch :    66 | Loss : 2942405.5\n",
      "Epoch :    67 | Loss : 2890822.0\n",
      "Epoch :    68 | Loss : 2839952.75\n",
      "Epoch :    69 | Loss : 2789792.25\n",
      "Epoch :    71 | Loss : 2691571.25\n",
      "Epoch :    72 | Loss : 2643497.75\n",
      "Epoch :    73 | Loss : 2596107.5\n",
      "Epoch :    74 | Loss : 2549394.25\n",
      "Epoch :    75 | Loss : 2503350.5\n",
      "Epoch :    76 | Loss : 2457971.5\n",
      "Epoch :    77 | Loss : 2413250.25\n",
      "Epoch :    78 | Loss : 2369180.25\n",
      "Epoch :    79 | Loss : 2325754.5\n",
      "Epoch :    81 | Loss : 2240814.25\n",
      "Epoch :    82 | Loss : 2199286.0\n",
      "Epoch :    83 | Loss : 2158377.5\n",
      "Epoch :    84 | Loss : 2118082.25\n",
      "Epoch :    85 | Loss : 2078394.25\n",
      "Epoch :    86 | Loss : 2039306.75\n",
      "Epoch :    87 | Loss : 2000814.125\n",
      "Epoch :    88 | Loss : 1962909.5\n",
      "Epoch :    89 | Loss : 1925587.0\n",
      "Epoch :    91 | Loss : 1852663.125\n",
      "Epoch :    92 | Loss : 1817050.125\n",
      "Epoch :    93 | Loss : 1781993.625\n",
      "Epoch :    94 | Loss : 1747489.0\n",
      "Epoch :    95 | Loss : 1713529.25\n",
      "Epoch :    96 | Loss : 1680108.75\n",
      "Epoch :    97 | Loss : 1647221.0\n",
      "Epoch :    98 | Loss : 1614860.25\n",
      "Epoch :    99 | Loss : 1583020.125\n",
      "Epoch :   101 | Loss : 1520879.125\n",
      "Epoch :   102 | Loss : 1490565.875\n",
      "Epoch :   103 | Loss : 1460750.375\n",
      "Epoch :   104 | Loss : 1431425.625\n",
      "Epoch :   105 | Loss : 1402586.5\n",
      "Epoch :   106 | Loss : 1374226.75\n",
      "Epoch :   107 | Loss : 1346340.75\n",
      "Epoch :   108 | Loss : 1318922.625\n",
      "Epoch :   109 | Loss : 1291967.0\n",
      "Epoch :   111 | Loss : 1239419.75\n",
      "Epoch :   112 | Loss : 1213816.5\n",
      "Epoch :   113 | Loss : 1188653.375\n",
      "Epoch :   114 | Loss : 1163924.0\n",
      "Epoch :   115 | Loss : 1139623.25\n",
      "Epoch :   116 | Loss : 1115745.625\n",
      "Epoch :   117 | Loss : 1092285.25\n",
      "Epoch :   118 | Loss : 1069237.125\n",
      "Epoch :   119 | Loss : 1046595.625\n",
      "Epoch :   121 | Loss : 1002511.375\n",
      "Epoch :   122 | Loss : 981057.875\n",
      "Epoch :   123 | Loss : 959989.875\n",
      "Epoch :   124 | Loss : 939301.9375\n",
      "Epoch :   125 | Loss : 918988.9375\n",
      "Epoch :   126 | Loss : 899045.9375\n",
      "Epoch :   127 | Loss : 879467.5625\n",
      "Epoch :   128 | Loss : 860248.9375\n",
      "Epoch :   129 | Loss : 841384.9375\n",
      "Epoch :   131 | Loss : 804700.8125\n",
      "Epoch :   132 | Loss : 786870.8125\n",
      "Epoch :   133 | Loss : 769375.5625\n",
      "Epoch :   134 | Loss : 752210.4375\n",
      "Epoch :   135 | Loss : 735370.4375\n",
      "Epoch :   136 | Loss : 718850.75\n",
      "Epoch :   137 | Loss : 702646.875\n",
      "Epoch :   138 | Loss : 686754.0625\n",
      "Epoch :   139 | Loss : 671167.5625\n",
      "Epoch :   141 | Loss : 640895.1875\n",
      "Epoch :   142 | Loss : 626200.375\n",
      "Epoch :   143 | Loss : 611793.5625\n",
      "Epoch :   144 | Loss : 597670.6875\n",
      "Epoch :   145 | Loss : 583827.1875\n",
      "Epoch :   146 | Loss : 570258.5625\n",
      "Epoch :   147 | Loss : 556960.6875\n",
      "Epoch :   148 | Loss : 543929.3125\n",
      "Epoch :   149 | Loss : 531160.125\n",
      "Epoch :   151 | Loss : 506391.4375\n",
      "Epoch :   152 | Loss : 494383.6875\n",
      "Epoch :   153 | Loss : 482621.71875\n",
      "Epoch :   154 | Loss : 471101.40625\n",
      "Epoch :   155 | Loss : 459818.71875\n",
      "Epoch :   156 | Loss : 448769.875\n",
      "Epoch :   157 | Loss : 437950.90625\n",
      "Epoch :   158 | Loss : 427357.875\n",
      "Epoch :   159 | Loss : 416987.09375\n",
      "Epoch :   161 | Loss : 396897.1875\n",
      "Epoch :   162 | Loss : 387170.59375\n",
      "Epoch :   163 | Loss : 377651.375\n",
      "Epoch :   164 | Loss : 368336.0\n",
      "Epoch :   165 | Loss : 359220.9375\n",
      "Epoch :   166 | Loss : 350302.5625\n",
      "Epoch :   167 | Loss : 341577.4375\n",
      "Epoch :   168 | Loss : 333042.21875\n",
      "Epoch :   169 | Loss : 324693.46875\n",
      "Epoch :   171 | Loss : 308542.03125\n",
      "Epoch :   172 | Loss : 300732.78125\n",
      "Epoch :   173 | Loss : 293096.90625\n",
      "Epoch :   174 | Loss : 285631.15625\n",
      "Epoch :   175 | Loss : 278332.46875\n",
      "Epoch :   176 | Loss : 271197.75\n",
      "Epoch :   177 | Loss : 264223.84375\n",
      "Epoch :   178 | Loss : 257407.859375\n",
      "Epoch :   179 | Loss : 250746.796875\n",
      "Epoch :   181 | Loss : 237877.640625\n",
      "Epoch :   182 | Loss : 231663.859375\n",
      "Epoch :   183 | Loss : 225593.484375\n",
      "Epoch :   184 | Loss : 219663.796875\n",
      "Epoch :   185 | Loss : 213871.96875\n",
      "Epoch :   186 | Loss : 208215.40625\n",
      "Epoch :   187 | Loss : 202691.40625\n",
      "Epoch :   188 | Loss : 197297.375\n",
      "Epoch :   189 | Loss : 192030.71875\n",
      "Epoch :   191 | Loss : 181869.453125\n",
      "Epoch :   192 | Loss : 176969.859375\n",
      "Epoch :   193 | Loss : 172187.734375\n",
      "Epoch :   194 | Loss : 167520.65625\n",
      "Epoch :   195 | Loss : 162966.3125\n",
      "Epoch :   196 | Loss : 158522.375\n",
      "Epoch :   197 | Loss : 154186.53125\n",
      "Epoch :   198 | Loss : 149956.625\n",
      "Epoch :   199 | Loss : 145830.34375\n",
      "Epoch :   201 | Loss : 137880.15625\n",
      "Epoch :   202 | Loss : 134052.015625\n",
      "Epoch :   203 | Loss : 130319.09375\n",
      "Epoch :   204 | Loss : 126679.34375\n",
      "Epoch :   205 | Loss : 123130.7578125\n",
      "Epoch :   206 | Loss : 119671.359375\n",
      "Epoch :   207 | Loss : 116299.2734375\n",
      "Epoch :   208 | Loss : 113012.5625\n",
      "Epoch :   209 | Loss : 109809.34375\n",
      "Epoch :   211 | Loss : 103646.15625\n",
      "Epoch :   212 | Loss : 100682.5859375\n",
      "Epoch :   213 | Loss : 97795.4296875\n",
      "Epoch :   214 | Loss : 94982.890625\n",
      "Epoch :   215 | Loss : 92243.3671875\n",
      "Epoch :   216 | Loss : 89575.1640625\n",
      "Epoch :   217 | Loss : 86976.6953125\n",
      "Epoch :   218 | Loss : 84446.34375\n",
      "Epoch :   219 | Loss : 81982.5859375\n",
      "Epoch :   221 | Loss : 77248.6875\n",
      "Epoch :   222 | Loss : 74975.5859375\n",
      "Epoch :   223 | Loss : 72763.1328125\n",
      "Epoch :   224 | Loss : 70609.859375\n",
      "Epoch :   225 | Loss : 68514.4375\n",
      "Epoch :   226 | Loss : 66475.46875\n",
      "Epoch :   227 | Loss : 64491.61328125\n",
      "Epoch :   228 | Loss : 62561.5703125\n",
      "Epoch :   229 | Loss : 60684.05859375\n",
      "Epoch :   231 | Loss : 57081.62109375\n",
      "Epoch :   232 | Loss : 55354.234375\n",
      "Epoch :   233 | Loss : 53674.47265625\n",
      "Epoch :   234 | Loss : 52041.19140625\n",
      "Epoch :   235 | Loss : 50453.2578125\n",
      "Epoch :   236 | Loss : 48909.5390625\n",
      "Epoch :   237 | Loss : 47408.9453125\n",
      "Epoch :   238 | Loss : 45950.421875\n",
      "Epoch :   239 | Loss : 44532.9140625\n",
      "Epoch :   241 | Loss : 41816.8515625\n",
      "Epoch :   242 | Loss : 40516.32421875\n",
      "Epoch :   243 | Loss : 39252.83203125\n",
      "Epoch :   244 | Loss : 38025.44921875\n",
      "Epoch :   245 | Loss : 36833.23828125\n",
      "Epoch :   246 | Loss : 35675.3125\n",
      "Epoch :   247 | Loss : 34550.78515625\n",
      "Epoch :   248 | Loss : 33458.79296875\n",
      "Epoch :   249 | Loss : 32398.5\n",
      "Epoch :   251 | Loss : 30369.728515625\n",
      "Epoch :   252 | Loss : 29399.65234375\n",
      "Epoch :   253 | Loss : 28458.083984375\n",
      "Epoch :   254 | Loss : 27544.271484375\n",
      "Epoch :   255 | Loss : 26657.482421875\n",
      "Epoch :   256 | Loss : 25796.99609375\n",
      "Epoch :   257 | Loss : 24962.1015625\n",
      "Epoch :   258 | Loss : 24152.12890625\n",
      "Epoch :   259 | Loss : 23366.396484375\n",
      "Epoch :   261 | Loss : 21865.056640625\n",
      "Epoch :   262 | Loss : 21148.18359375\n",
      "Epoch :   263 | Loss : 20453.029296875\n",
      "Epoch :   264 | Loss : 19778.99609375\n",
      "Epoch :   265 | Loss : 19125.498046875\n",
      "Epoch :   266 | Loss : 18491.98046875\n",
      "Epoch :   267 | Loss : 17877.880859375\n",
      "Epoch :   268 | Loss : 17282.662109375\n",
      "Epoch :   269 | Loss : 16705.796875\n",
      "Epoch :   271 | Loss : 15605.083984375\n",
      "Epoch :   272 | Loss : 15080.24609375\n",
      "Epoch :   273 | Loss : 14571.7822265625\n",
      "Epoch :   274 | Loss : 14079.228515625\n",
      "Epoch :   275 | Loss : 13602.126953125\n",
      "Epoch :   276 | Loss : 13140.0419921875\n",
      "Epoch :   277 | Loss : 12692.5380859375\n",
      "Epoch :   278 | Loss : 12259.1982421875\n",
      "Epoch :   279 | Loss : 11839.6123046875\n",
      "Epoch :   281 | Loss : 11040.1162109375\n",
      "Epoch :   282 | Loss : 10659.44140625\n",
      "Epoch :   283 | Loss : 10290.9853515625\n",
      "Epoch :   284 | Loss : 9934.3896484375\n",
      "Epoch :   285 | Loss : 9589.3056640625\n",
      "Epoch :   286 | Loss : 9255.39453125\n",
      "Epoch :   287 | Loss : 8932.3212890625\n",
      "Epoch :   288 | Loss : 8619.7666015625\n",
      "Epoch :   289 | Loss : 8317.412109375\n",
      "Epoch :   291 | Loss : 7742.09912109375\n",
      "Epoch :   292 | Loss : 7468.552734375\n",
      "Epoch :   293 | Loss : 7204.03271484375\n",
      "Epoch :   294 | Loss : 6948.26904296875\n",
      "Epoch :   295 | Loss : 6700.9912109375\n",
      "Epoch :   296 | Loss : 6461.94287109375\n",
      "Epoch :   297 | Loss : 6230.86962890625\n",
      "Epoch :   298 | Loss : 6007.52685546875\n",
      "Epoch :   299 | Loss : 5791.6767578125\n",
      "Epoch :   301 | Loss : 5381.53173828125\n",
      "Epoch :   302 | Loss : 5186.79248046875\n",
      "Epoch :   303 | Loss : 4998.6572265625\n",
      "Epoch :   304 | Loss : 4816.9169921875\n",
      "Epoch :   305 | Loss : 4641.37158203125\n",
      "Epoch :   306 | Loss : 4471.82763671875\n",
      "Epoch :   307 | Loss : 4308.09228515625\n",
      "Epoch :   308 | Loss : 4149.98388671875\n",
      "Epoch :   309 | Loss : 3997.32177734375\n",
      "Epoch :   311 | Loss : 3707.64697265625\n",
      "Epoch :   312 | Loss : 3570.30224609375\n",
      "Epoch :   313 | Loss : 3437.738037109375\n",
      "Epoch :   314 | Loss : 3309.800537109375\n",
      "Epoch :   315 | Loss : 3186.340087890625\n",
      "Epoch :   316 | Loss : 3067.21044921875\n",
      "Epoch :   317 | Loss : 2952.27099609375\n",
      "Epoch :   318 | Loss : 2841.38427734375\n",
      "Epoch :   319 | Loss : 2734.418212890625\n",
      "Epoch :   321 | Loss : 2531.733642578125\n",
      "Epoch :   322 | Loss : 2435.769775390625\n",
      "Epoch :   323 | Loss : 2343.23291015625\n",
      "Epoch :   324 | Loss : 2254.00927734375\n",
      "Epoch :   325 | Loss : 2167.98876953125\n",
      "Epoch :   326 | Loss : 2085.0634765625\n",
      "Epoch :   327 | Loss : 2005.130126953125\n",
      "Epoch :   328 | Loss : 1928.0875244140625\n",
      "Epoch :   329 | Loss : 1853.8382568359375\n",
      "Epoch :   331 | Loss : 1713.3446044921875\n",
      "Epoch :   332 | Loss : 1646.9200439453125\n",
      "Epoch :   333 | Loss : 1582.9278564453125\n",
      "Epoch :   334 | Loss : 1521.2847900390625\n",
      "Epoch :   335 | Loss : 1461.9100341796875\n",
      "Epoch :   336 | Loss : 1404.725830078125\n",
      "Epoch :   337 | Loss : 1349.65673828125\n",
      "Epoch :   338 | Loss : 1296.6290283203125\n",
      "Epoch :   339 | Loss : 1245.572021484375\n",
      "Epoch :   341 | Loss : 1149.09814453125\n",
      "Epoch :   342 | Loss : 1103.5504150390625\n",
      "Epoch :   343 | Loss : 1059.7117919921875\n",
      "Epoch :   344 | Loss : 1017.5223388671875\n",
      "Epoch :   345 | Loss : 976.9239501953125\n",
      "Epoch :   346 | Loss : 937.8599853515625\n",
      "Epoch :   347 | Loss : 900.2761840820312\n",
      "Epoch :   348 | Loss : 864.119873046875\n",
      "Epoch :   349 | Loss : 829.3402099609375\n",
      "Epoch :   351 | Loss : 763.7149047851562\n",
      "Epoch :   352 | Loss : 732.77587890625\n",
      "Epoch :   353 | Loss : 703.0260009765625\n",
      "Epoch :   354 | Loss : 674.4223022460938\n",
      "Epoch :   355 | Loss : 646.9232177734375\n",
      "Epoch :   356 | Loss : 620.488525390625\n",
      "Epoch :   357 | Loss : 595.07958984375\n",
      "Epoch :   358 | Loss : 570.6588745117188\n",
      "Epoch :   359 | Loss : 547.190185546875\n",
      "Epoch :   361 | Loss : 502.9699401855469\n",
      "Epoch :   362 | Loss : 482.15203857421875\n",
      "Epoch :   363 | Loss : 462.1533508300781\n",
      "Epoch :   364 | Loss : 442.943359375\n",
      "Epoch :   365 | Loss : 424.49285888671875\n",
      "Epoch :   366 | Loss : 406.7734069824219\n",
      "Epoch :   367 | Loss : 389.75762939453125\n",
      "Epoch :   368 | Loss : 373.419189453125\n",
      "Epoch :   369 | Loss : 357.73260498046875\n",
      "Epoch :   371 | Loss : 328.2176818847656\n",
      "Epoch :   372 | Loss : 314.3426818847656\n",
      "Epoch :   373 | Loss : 301.0263671875\n",
      "Epoch :   374 | Loss : 288.24749755859375\n",
      "Epoch :   375 | Loss : 275.9854736328125\n",
      "Epoch :   376 | Loss : 264.22052001953125\n",
      "Epoch :   377 | Loss : 252.93365478515625\n",
      "Epoch :   378 | Loss : 242.10638427734375\n",
      "Epoch :   379 | Loss : 231.72103881835938\n",
      "Epoch :   381 | Loss : 212.20848083496094\n",
      "Epoch :   382 | Loss : 203.0488739013672\n",
      "Epoch :   383 | Loss : 194.26654052734375\n",
      "Epoch :   384 | Loss : 185.8466796875\n",
      "Epoch :   385 | Loss : 177.77516174316406\n",
      "Epoch :   386 | Loss : 170.0382843017578\n",
      "Epoch :   387 | Loss : 162.62295532226562\n",
      "Epoch :   388 | Loss : 155.5164031982422\n",
      "Epoch :   389 | Loss : 148.7064971923828\n",
      "Epoch :   391 | Loss : 135.9299774169922\n",
      "Epoch :   392 | Loss : 129.941162109375\n",
      "Epoch :   393 | Loss : 124.20452880859375\n",
      "Epoch :   394 | Loss : 118.70999145507812\n",
      "Epoch :   395 | Loss : 113.4478530883789\n",
      "Epoch :   396 | Loss : 108.40877532958984\n",
      "Epoch :   397 | Loss : 103.58377075195312\n",
      "Epoch :   398 | Loss : 98.96418762207031\n",
      "Epoch :   399 | Loss : 94.5417251586914\n",
      "Epoch :   401 | Loss : 86.25639343261719\n",
      "Epoch :   402 | Loss : 82.37847137451172\n",
      "Epoch :   403 | Loss : 78.66744995117188\n",
      "Epoch :   404 | Loss : 75.11650848388672\n",
      "Epoch :   405 | Loss : 71.71905517578125\n",
      "Epoch :   406 | Loss : 68.46879577636719\n",
      "Epoch :   407 | Loss : 65.35963439941406\n",
      "Epoch :   408 | Loss : 62.38576889038086\n",
      "Epoch :   409 | Loss : 59.54155349731445\n",
      "Epoch :   411 | Loss : 54.22079849243164\n",
      "Epoch :   412 | Loss : 51.73411560058594\n",
      "Epoch :   413 | Loss : 49.356788635253906\n",
      "Epoch :   414 | Loss : 47.08423614501953\n",
      "Epoch :   415 | Loss : 44.91205978393555\n",
      "Epoch :   416 | Loss : 42.83602523803711\n",
      "Epoch :   417 | Loss : 40.85206604003906\n",
      "Epoch :   418 | Loss : 38.956302642822266\n",
      "Epoch :   419 | Loss : 37.14498519897461\n",
      "Epoch :   421 | Loss : 33.7614631652832\n",
      "Epoch :   422 | Loss : 32.18251037597656\n",
      "Epoch :   423 | Loss : 30.67449378967285\n",
      "Epoch :   424 | Loss : 29.23436164855957\n",
      "Epoch :   425 | Loss : 27.85919761657715\n",
      "Epoch :   426 | Loss : 26.546205520629883\n",
      "Epoch :   427 | Loss : 25.29269027709961\n",
      "Epoch :   428 | Loss : 24.096086502075195\n",
      "Epoch :   429 | Loss : 22.953920364379883\n",
      "Epoch :   431 | Loss : 20.823528289794922\n",
      "Epoch :   432 | Loss : 19.830856323242188\n",
      "Epoch :   433 | Loss : 18.88372802734375\n",
      "Epoch :   434 | Loss : 17.980138778686523\n",
      "Epoch :   435 | Loss : 17.118173599243164\n",
      "Epoch :   436 | Loss : 16.296001434326172\n",
      "Epoch :   437 | Loss : 15.511859893798828\n",
      "Epoch :   438 | Loss : 14.764069557189941\n",
      "Epoch :   439 | Loss : 14.05101490020752\n",
      "Epoch :   441 | Loss : 12.722999572753906\n",
      "Epoch :   442 | Loss : 12.105141639709473\n",
      "Epoch :   443 | Loss : 11.516222953796387\n",
      "Epoch :   444 | Loss : 10.95494556427002\n",
      "Epoch :   445 | Loss : 10.420064926147461\n",
      "Epoch :   446 | Loss : 9.910391807556152\n",
      "Epoch :   447 | Loss : 9.424787521362305\n",
      "Epoch :   448 | Loss : 8.962162017822266\n",
      "Epoch :   449 | Loss : 8.521475791931152\n",
      "Epoch :   451 | Loss : 7.701964378356934\n",
      "Epoch :   452 | Loss : 7.321275234222412\n",
      "Epoch :   453 | Loss : 6.958785533905029\n",
      "Epoch :   454 | Loss : 6.613661766052246\n",
      "Epoch :   455 | Loss : 6.285104751586914\n",
      "Epoch :   456 | Loss : 5.972352027893066\n",
      "Epoch :   457 | Loss : 5.674674034118652\n",
      "Epoch :   458 | Loss : 5.391373157501221\n",
      "Epoch :   459 | Loss : 5.121783256530762\n",
      "Epoch :   461 | Loss : 4.621213912963867\n",
      "Epoch :   462 | Loss : 4.389044761657715\n",
      "Epoch :   463 | Loss : 4.1682024002075195\n",
      "Epoch :   464 | Loss : 3.958158016204834\n",
      "Epoch :   465 | Loss : 3.7584028244018555\n",
      "Epoch :   466 | Loss : 3.5684525966644287\n",
      "Epoch :   467 | Loss : 3.387845039367676\n",
      "Epoch :   468 | Loss : 3.216139078140259\n",
      "Epoch :   469 | Loss : 3.052912712097168\n",
      "Epoch :   471 | Loss : 2.7503061294555664\n",
      "Epoch :   472 | Loss : 2.610175371170044\n",
      "Epoch :   473 | Loss : 2.477020740509033\n",
      "Epoch :   474 | Loss : 2.3505077362060547\n",
      "Epoch :   475 | Loss : 2.230318784713745\n",
      "Epoch :   476 | Loss : 2.1161491870880127\n",
      "Epoch :   477 | Loss : 2.007708787918091\n",
      "Epoch :   478 | Loss : 1.9047210216522217\n",
      "Epoch :   479 | Loss : 1.8069219589233398\n",
      "Epoch :   481 | Loss : 1.6258965730667114\n",
      "Epoch :   482 | Loss : 1.5422015190124512\n",
      "Epoch :   483 | Loss : 1.462756633758545\n",
      "Epoch :   484 | Loss : 1.3873543739318848\n",
      "Epoch :   485 | Loss : 1.315797209739685\n",
      "Epoch :   486 | Loss : 1.2478963136672974\n",
      "Epoch :   487 | Loss : 1.1834715604782104\n",
      "Epoch :   488 | Loss : 1.122351050376892\n",
      "Epoch :   489 | Loss : 1.064372181892395\n",
      "Epoch :   491 | Loss : 0.9572235345840454\n",
      "Epoch :   492 | Loss : 0.9077650904655457\n",
      "Epoch :   493 | Loss : 0.8608684539794922\n",
      "Epoch :   494 | Loss : 0.8164060115814209\n",
      "Epoch :   495 | Loss : 0.7742564678192139\n",
      "Epoch :   496 | Loss : 0.7343031764030457\n",
      "Epoch :   497 | Loss : 0.6964364647865295\n",
      "Epoch :   498 | Loss : 0.6605504751205444\n",
      "Epoch :   499 | Loss : 0.6265460848808289\n",
      "Epoch :   501 | Loss : 0.5638052821159363\n",
      "Epoch :   502 | Loss : 0.5348922610282898\n",
      "Epoch :   503 | Loss : 0.5075069069862366\n",
      "Epoch :   504 | Loss : 0.4815714657306671\n",
      "Epoch :   505 | Loss : 0.45701172947883606\n",
      "Epoch :   506 | Loss : 0.4337575137615204\n",
      "Epoch :   507 | Loss : 0.4117415249347687\n",
      "Epoch :   508 | Loss : 0.39090022444725037\n",
      "Epoch :   509 | Loss : 0.3711736798286438\n",
      "Epoch :   511 | Loss : 0.33483579754829407\n",
      "Epoch :   512 | Loss : 0.3181180953979492\n",
      "Epoch :   513 | Loss : 0.3023013472557068\n",
      "Epoch :   514 | Loss : 0.2873384654521942\n",
      "Epoch :   515 | Loss : 0.27318504452705383\n",
      "Epoch :   516 | Loss : 0.25979891419410706\n",
      "Epoch :   517 | Loss : 0.2471398115158081\n",
      "Epoch :   518 | Loss : 0.2351693958044052\n",
      "Epoch :   519 | Loss : 0.22385168075561523\n",
      "Epoch :   521 | Loss : 0.2030385434627533\n",
      "Epoch :   522 | Loss : 0.19347956776618958\n",
      "Epoch :   523 | Loss : 0.18444567918777466\n",
      "Epoch :   524 | Loss : 0.17590922117233276\n",
      "Epoch :   525 | Loss : 0.16784381866455078\n",
      "Epoch :   526 | Loss : 0.16022422909736633\n",
      "Epoch :   527 | Loss : 0.15302644670009613\n",
      "Epoch :   528 | Loss : 0.14622808992862701\n",
      "Epoch :   529 | Loss : 0.13980770111083984\n",
      "Epoch :   531 | Loss : 0.12802082300186157\n",
      "Epoch :   532 | Loss : 0.12261661887168884\n",
      "Epoch :   533 | Loss : 0.11751511693000793\n",
      "Epoch :   534 | Loss : 0.11270012706518173\n",
      "Epoch :   535 | Loss : 0.10815609246492386\n",
      "Epoch :   536 | Loss : 0.10386808216571808\n",
      "Epoch :   537 | Loss : 0.0998222604393959\n",
      "Epoch :   538 | Loss : 0.09600533545017242\n",
      "Epoch :   539 | Loss : 0.0924047976732254\n",
      "Epoch :   541 | Loss : 0.08580604195594788\n",
      "Epoch :   542 | Loss : 0.08278600126504898\n",
      "Epoch :   543 | Loss : 0.07993850111961365\n",
      "Epoch :   544 | Loss : 0.07725391536951065\n",
      "Epoch :   545 | Loss : 0.07472346723079681\n",
      "Epoch :   546 | Loss : 0.07233832031488419\n",
      "Epoch :   547 | Loss : 0.07009053230285645\n",
      "Epoch :   548 | Loss : 0.06797250360250473\n",
      "Epoch :   549 | Loss : 0.06597676128149033\n",
      "Epoch :   551 | Loss : 0.062325749546289444\n",
      "Epoch :   552 | Loss : 0.06065778061747551\n",
      "Epoch :   553 | Loss : 0.05908698961138725\n",
      "Epoch :   554 | Loss : 0.05760779604315758\n",
      "Epoch :   555 | Loss : 0.05621521174907684\n",
      "Epoch :   556 | Loss : 0.054904188960790634\n",
      "Epoch :   557 | Loss : 0.05367012694478035\n",
      "Epoch :   558 | Loss : 0.052508722990751266\n",
      "Epoch :   559 | Loss : 0.05141575634479523\n",
      "Epoch :   561 | Loss : 0.049419697374105453\n",
      "Epoch :   562 | Loss : 0.04850950092077255\n",
      "Epoch :   563 | Loss : 0.04765336215496063\n",
      "Epoch :   564 | Loss : 0.04684818908572197\n",
      "Epoch :   565 | Loss : 0.0460909940302372\n",
      "Epoch :   566 | Loss : 0.045379068702459335\n",
      "Epoch :   567 | Loss : 0.04470974579453468\n",
      "Epoch :   568 | Loss : 0.04408057779073715\n",
      "Epoch :   569 | Loss : 0.04348921403288841\n",
      "Epoch :   571 | Loss : 0.04241127893328667\n",
      "Epoch :   572 | Loss : 0.04192062467336655\n",
      "Epoch :   573 | Loss : 0.041459716856479645\n",
      "Epoch :   574 | Loss : 0.0410267636179924\n",
      "Epoch :   575 | Loss : 0.04062015190720558\n",
      "Epoch :   576 | Loss : 0.04023829847574234\n",
      "Epoch :   577 | Loss : 0.03987978398799896\n",
      "Epoch :   578 | Loss : 0.039543166756629944\n",
      "Epoch :   579 | Loss : 0.0392271913588047\n",
      "Epoch :   581 | Loss : 0.038652293384075165\n",
      "Epoch :   582 | Loss : 0.03839111328125\n",
      "Epoch :   583 | Loss : 0.038146089762449265\n",
      "Epoch :   584 | Loss : 0.03791623190045357\n",
      "Epoch :   585 | Loss : 0.037700630724430084\n",
      "Epoch :   586 | Loss : 0.03749840706586838\n",
      "Epoch :   587 | Loss : 0.03730878233909607\n",
      "Epoch :   588 | Loss : 0.03713097795844078\n",
      "Epoch :   589 | Loss : 0.036964308470487595\n",
      "Epoch :   591 | Loss : 0.036661598831415176\n",
      "Epoch :   592 | Loss : 0.03652435913681984\n",
      "Epoch :   593 | Loss : 0.036395762115716934\n",
      "Epoch :   594 | Loss : 0.03627527132630348\n",
      "Epoch :   595 | Loss : 0.036162421107292175\n",
      "Epoch :   596 | Loss : 0.03605670854449272\n",
      "Epoch :   597 | Loss : 0.03595771640539169\n",
      "Epoch :   598 | Loss : 0.035865020006895065\n",
      "Epoch :   599 | Loss : 0.035778217017650604\n",
      "Epoch :   601 | Loss : 0.03562092408537865\n",
      "Epoch :   602 | Loss : 0.035549767315387726\n",
      "Epoch :   603 | Loss : 0.035483162850141525\n",
      "Epoch :   604 | Loss : 0.03542085364460945\n",
      "Epoch :   605 | Loss : 0.035362571477890015\n",
      "Epoch :   606 | Loss : 0.03530803695321083\n",
      "Epoch :   607 | Loss : 0.035257045179605484\n",
      "Epoch :   608 | Loss : 0.035209354013204575\n",
      "Epoch :   609 | Loss : 0.03516476973891258\n",
      "Epoch :   611 | Loss : 0.035084132105112076\n",
      "Epoch :   612 | Loss : 0.03504772111773491\n",
      "Epoch :   613 | Loss : 0.035013698041439056\n",
      "Epoch :   614 | Loss : 0.03498188778758049\n",
      "Epoch :   615 | Loss : 0.03495220094919205\n",
      "Epoch :   616 | Loss : 0.0349244624376297\n",
      "Epoch :   617 | Loss : 0.0348985530436039\n",
      "Epoch :   618 | Loss : 0.03487435355782509\n",
      "Epoch :   619 | Loss : 0.03485177457332611\n",
      "Epoch :   621 | Loss : 0.03481099382042885\n",
      "Epoch :   622 | Loss : 0.03479261323809624\n",
      "Epoch :   623 | Loss : 0.03477548062801361\n",
      "Epoch :   624 | Loss : 0.03475948050618172\n",
      "Epoch :   625 | Loss : 0.034744564443826675\n",
      "Epoch :   626 | Loss : 0.034730665385723114\n",
      "Epoch :   627 | Loss : 0.03471766784787178\n",
      "Epoch :   628 | Loss : 0.03470556437969208\n",
      "Epoch :   629 | Loss : 0.034694284200668335\n",
      "Epoch :   631 | Loss : 0.03467395901679993\n",
      "Epoch :   632 | Loss : 0.034664832055568695\n",
      "Epoch :   633 | Loss : 0.03465631976723671\n",
      "Epoch :   634 | Loss : 0.03464837744832039\n",
      "Epoch :   635 | Loss : 0.03464099392294884\n",
      "Epoch :   636 | Loss : 0.03463410958647728\n",
      "Epoch :   637 | Loss : 0.034627705812454224\n",
      "Epoch :   638 | Loss : 0.034621745347976685\n",
      "Epoch :   639 | Loss : 0.03461618721485138\n",
      "Epoch :   641 | Loss : 0.0346062146127224\n",
      "Epoch :   642 | Loss : 0.03460172936320305\n",
      "Epoch :   643 | Loss : 0.03459758311510086\n",
      "Epoch :   644 | Loss : 0.034593693912029266\n",
      "Epoch :   645 | Loss : 0.034590091556310654\n",
      "Epoch :   646 | Loss : 0.034586746245622635\n",
      "Epoch :   647 | Loss : 0.034583624452352524\n",
      "Epoch :   648 | Loss : 0.03458072245121002\n",
      "Epoch :   649 | Loss : 0.03457803651690483\n",
      "Epoch :   651 | Loss : 0.034573208540678024\n",
      "Epoch :   652 | Loss : 0.03457104414701462\n",
      "Epoch :   653 | Loss : 0.03456904739141464\n",
      "Epoch :   654 | Loss : 0.03456718102097511\n",
      "Epoch :   655 | Loss : 0.03456544876098633\n",
      "Epoch :   656 | Loss : 0.03456385061144829\n",
      "Epoch :   657 | Loss : 0.034562353044748306\n",
      "Epoch :   658 | Loss : 0.03456097096204758\n",
      "Epoch :   659 | Loss : 0.03455968201160431\n",
      "Epoch :   661 | Loss : 0.03455738723278046\n",
      "Epoch :   662 | Loss : 0.03455635905265808\n",
      "Epoch :   663 | Loss : 0.034555405378341675\n",
      "Epoch :   664 | Loss : 0.03455452620983124\n",
      "Epoch :   665 | Loss : 0.03455371409654617\n",
      "Epoch :   666 | Loss : 0.03455294668674469\n",
      "Epoch :   667 | Loss : 0.03455224633216858\n",
      "Epoch :   668 | Loss : 0.03455159440636635\n",
      "Epoch :   669 | Loss : 0.034550994634628296\n",
      "Epoch :   671 | Loss : 0.03454991430044174\n",
      "Epoch :   672 | Loss : 0.03454944118857384\n",
      "Epoch :   673 | Loss : 0.03454899042844772\n",
      "Epoch :   674 | Loss : 0.034548576921224594\n",
      "Epoch :   675 | Loss : 0.034548208117485046\n",
      "Epoch :   676 | Loss : 0.03454785421490669\n",
      "Epoch :   677 | Loss : 0.03454752638936043\n",
      "Epoch :   678 | Loss : 0.03454722464084625\n",
      "Epoch :   679 | Loss : 0.03454694524407387\n",
      "Epoch :   681 | Loss : 0.03454645350575447\n",
      "Epoch :   682 | Loss : 0.03454622998833656\n",
      "Epoch :   683 | Loss : 0.034546028822660446\n",
      "Epoch :   684 | Loss : 0.03454584628343582\n",
      "Epoch :   685 | Loss : 0.0345456600189209\n",
      "Epoch :   686 | Loss : 0.034545496106147766\n",
      "Epoch :   687 | Loss : 0.03454536944627762\n",
      "Epoch :   688 | Loss : 0.03454521298408508\n",
      "Epoch :   689 | Loss : 0.034545090049505234\n",
      "Epoch :   691 | Loss : 0.03454487398266792\n",
      "Epoch :   692 | Loss : 0.03454476222395897\n",
      "Epoch :   693 | Loss : 0.034544672816991806\n",
      "Epoch :   694 | Loss : 0.03454459086060524\n",
      "Epoch :   695 | Loss : 0.03454452008008957\n",
      "Epoch :   696 | Loss : 0.034544438123703\n",
      "Epoch :   697 | Loss : 0.03454437851905823\n",
      "Epoch :   698 | Loss : 0.03454432263970375\n",
      "Epoch :   699 | Loss : 0.03454425930976868\n",
      "Epoch :   701 | Loss : 0.03454415500164032\n",
      "Epoch :   702 | Loss : 0.03454411402344704\n",
      "Epoch :   703 | Loss : 0.034544073045253754\n",
      "Epoch :   704 | Loss : 0.03454403206706047\n",
      "Epoch :   705 | Loss : 0.034543998539447784\n",
      "Epoch :   706 | Loss : 0.0345439687371254\n",
      "Epoch :   707 | Loss : 0.03454393893480301\n",
      "Epoch :   708 | Loss : 0.03454392030835152\n",
      "Epoch :   709 | Loss : 0.034543897956609726\n",
      "Epoch :   711 | Loss : 0.034543849527835846\n",
      "Epoch :   712 | Loss : 0.03454381972551346\n",
      "Epoch :   713 | Loss : 0.03454380854964256\n",
      "Epoch :   714 | Loss : 0.03454378619790077\n",
      "Epoch :   715 | Loss : 0.034543778747320175\n",
      "Epoch :   716 | Loss : 0.03454376012086868\n",
      "Epoch :   717 | Loss : 0.034543752670288086\n",
      "Epoch :   718 | Loss : 0.03454374149441719\n",
      "Epoch :   719 | Loss : 0.034543730318546295\n",
      "Epoch :   721 | Loss : 0.034543704241514206\n",
      "Epoch :   722 | Loss : 0.03454368934035301\n",
      "Epoch :   723 | Loss : 0.03454368934035301\n",
      "Epoch :   724 | Loss : 0.03454367816448212\n",
      "Epoch :   725 | Loss : 0.03454367443919182\n",
      "Epoch :   726 | Loss : 0.03454367071390152\n",
      "Epoch :   727 | Loss : 0.034543659538030624\n",
      "Epoch :   728 | Loss : 0.034543655812740326\n",
      "Epoch :   729 | Loss : 0.034543655812740326\n",
      "Epoch :   731 | Loss : 0.034543655812740326\n",
      "Epoch :   732 | Loss : 0.03454364463686943\n",
      "Epoch :   733 | Loss : 0.03454364463686943\n",
      "Epoch :   734 | Loss : 0.034543637186288834\n",
      "Epoch :   735 | Loss : 0.03454362973570824\n",
      "Epoch :   736 | Loss : 0.03454362973570824\n",
      "Epoch :   737 | Loss : 0.03454362973570824\n",
      "Epoch :   738 | Loss : 0.03454362228512764\n",
      "Epoch :   739 | Loss : 0.03454362601041794\n",
      "Epoch :   741 | Loss : 0.034543611109256744\n",
      "Epoch :   742 | Loss : 0.03454362228512764\n",
      "Epoch :   743 | Loss : 0.03454361855983734\n",
      "Epoch :   744 | Loss : 0.034543611109256744\n",
      "Epoch :   745 | Loss : 0.03454362228512764\n",
      "Epoch :   746 | Loss : 0.03454362228512764\n",
      "Epoch :   747 | Loss : 0.03454361855983734\n",
      "Epoch :   748 | Loss : 0.034543607383966446\n",
      "Epoch :   749 | Loss : 0.034543611109256744\n",
      "Epoch :   751 | Loss : 0.03454359993338585\n",
      "Epoch :   752 | Loss : 0.034543611109256744\n",
      "Epoch :   753 | Loss : 0.03454360365867615\n",
      "Epoch :   754 | Loss : 0.03454360365867615\n",
      "Epoch :   755 | Loss : 0.03454359993338585\n",
      "Epoch :   756 | Loss : 0.03454359993338585\n",
      "Epoch :   757 | Loss : 0.03454359620809555\n",
      "Epoch :   758 | Loss : 0.03454359248280525\n",
      "Epoch :   759 | Loss : 0.03454360365867615\n",
      "Epoch :   761 | Loss : 0.03454359993338585\n",
      "Epoch :   762 | Loss : 0.03454359993338585\n",
      "Epoch :   763 | Loss : 0.03454359993338585\n",
      "Epoch :   764 | Loss : 0.03454359620809555\n",
      "Epoch :   765 | Loss : 0.03454359248280525\n",
      "Epoch :   766 | Loss : 0.03454359993338585\n",
      "Epoch :   767 | Loss : 0.03454359248280525\n",
      "Epoch :   768 | Loss : 0.03454359620809555\n",
      "Epoch :   769 | Loss : 0.03454359248280525\n",
      "Epoch :   771 | Loss : 0.034543588757514954\n",
      "Epoch :   772 | Loss : 0.034543588757514954\n",
      "Epoch :   773 | Loss : 0.03454359248280525\n",
      "Epoch :   774 | Loss : 0.034543588757514954\n",
      "Epoch :   775 | Loss : 0.03454359620809555\n",
      "Epoch :   776 | Loss : 0.03454359620809555\n",
      "Epoch :   777 | Loss : 0.03454359248280525\n",
      "Epoch :   778 | Loss : 0.034543585032224655\n",
      "Epoch :   779 | Loss : 0.034543588757514954\n",
      "Epoch :   781 | Loss : 0.034543585032224655\n",
      "Epoch :   782 | Loss : 0.034543588757514954\n",
      "Epoch :   783 | Loss : 0.034543588757514954\n",
      "Epoch :   784 | Loss : 0.034543588757514954\n",
      "Epoch :   785 | Loss : 0.034543585032224655\n",
      "Epoch :   786 | Loss : 0.034543588757514954\n",
      "Epoch :   787 | Loss : 0.03454359248280525\n",
      "Epoch :   788 | Loss : 0.034543588757514954\n",
      "Epoch :   789 | Loss : 0.034543585032224655\n",
      "Epoch :   791 | Loss : 0.03454358130693436\n",
      "Epoch :   792 | Loss : 0.034543585032224655\n",
      "Epoch :   793 | Loss : 0.034543588757514954\n",
      "Epoch :   794 | Loss : 0.03454358130693436\n",
      "Epoch :   795 | Loss : 0.03454358130693436\n",
      "Epoch :   796 | Loss : 0.03454358130693436\n",
      "Epoch :   797 | Loss : 0.03454358130693436\n",
      "Epoch :   798 | Loss : 0.034543585032224655\n",
      "Epoch :   799 | Loss : 0.03454358130693436\n",
      "Epoch :   801 | Loss : 0.03454357758164406\n",
      "Epoch :   802 | Loss : 0.03454357758164406\n",
      "Epoch :   803 | Loss : 0.03454358130693436\n",
      "Epoch :   804 | Loss : 0.03454356640577316\n",
      "Epoch :   805 | Loss : 0.03454357758164406\n",
      "Epoch :   806 | Loss : 0.03454358130693436\n",
      "Epoch :   807 | Loss : 0.03454358130693436\n",
      "Epoch :   808 | Loss : 0.034543588757514954\n",
      "Epoch :   809 | Loss : 0.03454358130693436\n",
      "Epoch :   811 | Loss : 0.034543562680482864\n",
      "Epoch :   812 | Loss : 0.03454356640577316\n",
      "Epoch :   813 | Loss : 0.03454357758164406\n",
      "Epoch :   814 | Loss : 0.03454356640577316\n",
      "Epoch :   815 | Loss : 0.03454357013106346\n",
      "Epoch :   816 | Loss : 0.03454358130693436\n",
      "Epoch :   817 | Loss : 0.03454358130693436\n",
      "Epoch :   818 | Loss : 0.03454358130693436\n",
      "Epoch :   819 | Loss : 0.034543558955192566\n",
      "Epoch :   821 | Loss : 0.03454357013106346\n",
      "Epoch :   822 | Loss : 0.03454357013106346\n",
      "Epoch :   823 | Loss : 0.03454357758164406\n",
      "Epoch :   824 | Loss : 0.03454357758164406\n",
      "Epoch :   825 | Loss : 0.03454356640577316\n",
      "Epoch :   826 | Loss : 0.034543562680482864\n",
      "Epoch :   827 | Loss : 0.034543562680482864\n",
      "Epoch :   828 | Loss : 0.03454356640577316\n",
      "Epoch :   829 | Loss : 0.03454357013106346\n",
      "Epoch :   831 | Loss : 0.03454357013106346\n",
      "Epoch :   832 | Loss : 0.03454356640577316\n",
      "Epoch :   833 | Loss : 0.03454356640577316\n",
      "Epoch :   834 | Loss : 0.03454356640577316\n",
      "Epoch :   835 | Loss : 0.03454356640577316\n",
      "Epoch :   836 | Loss : 0.03454356640577316\n",
      "Epoch :   837 | Loss : 0.03454356640577316\n",
      "Epoch :   838 | Loss : 0.03454356640577316\n",
      "Epoch :   839 | Loss : 0.03454356640577316\n",
      "Epoch :   841 | Loss : 0.03454356640577316\n",
      "Epoch :   842 | Loss : 0.03454356640577316\n",
      "Epoch :   843 | Loss : 0.03454356640577316\n",
      "Epoch :   844 | Loss : 0.034543562680482864\n",
      "Epoch :   845 | Loss : 0.034543562680482864\n",
      "Epoch :   846 | Loss : 0.034543562680482864\n",
      "Epoch :   847 | Loss : 0.034543562680482864\n",
      "Epoch :   848 | Loss : 0.034543562680482864\n",
      "Epoch :   849 | Loss : 0.034543562680482864\n",
      "Epoch :   851 | Loss : 0.034543562680482864\n",
      "Epoch :   852 | Loss : 0.034543558955192566\n",
      "Epoch :   853 | Loss : 0.034543558955192566\n",
      "Epoch :   854 | Loss : 0.034543558955192566\n",
      "Epoch :   855 | Loss : 0.034543558955192566\n",
      "Epoch :   856 | Loss : 0.034543562680482864\n",
      "Epoch :   857 | Loss : 0.034543562680482864\n",
      "Epoch :   858 | Loss : 0.034543562680482864\n",
      "Epoch :   859 | Loss : 0.034543562680482864\n",
      "Epoch :   861 | Loss : 0.03454355522990227\n",
      "Epoch :   862 | Loss : 0.03454355522990227\n",
      "Epoch :   863 | Loss : 0.03454355522990227\n",
      "Epoch :   864 | Loss : 0.03454355522990227\n",
      "Epoch :   865 | Loss : 0.03454355522990227\n",
      "Epoch :   866 | Loss : 0.03454355522990227\n",
      "Epoch :   867 | Loss : 0.034543540328741074\n",
      "Epoch :   868 | Loss : 0.03454355522990227\n",
      "Epoch :   869 | Loss : 0.03454355522990227\n",
      "Epoch :   871 | Loss : 0.03454354777932167\n",
      "Epoch :   872 | Loss : 0.03454354777932167\n",
      "Epoch :   873 | Loss : 0.03454354777932167\n",
      "Epoch :   874 | Loss : 0.03454354777932167\n",
      "Epoch :   875 | Loss : 0.03454354777932167\n",
      "Epoch :   876 | Loss : 0.03454354777932167\n",
      "Epoch :   877 | Loss : 0.03454354777932167\n",
      "Epoch :   878 | Loss : 0.03454354777932167\n",
      "Epoch :   879 | Loss : 0.03454354777932167\n",
      "Epoch :   881 | Loss : 0.03454354777932167\n",
      "Epoch :   882 | Loss : 0.03454354777932167\n",
      "Epoch :   883 | Loss : 0.034543540328741074\n",
      "Epoch :   884 | Loss : 0.03454354777932167\n",
      "Epoch :   885 | Loss : 0.03454355150461197\n",
      "Epoch :   886 | Loss : 0.034543540328741074\n",
      "Epoch :   887 | Loss : 0.034543540328741074\n",
      "Epoch :   888 | Loss : 0.034543540328741074\n",
      "Epoch :   889 | Loss : 0.034543540328741074\n",
      "Epoch :   891 | Loss : 0.03454352915287018\n",
      "Epoch :   892 | Loss : 0.03454352915287018\n",
      "Epoch :   893 | Loss : 0.03454352915287018\n",
      "Epoch :   894 | Loss : 0.03454352915287018\n",
      "Epoch :   895 | Loss : 0.03454352542757988\n",
      "Epoch :   896 | Loss : 0.03454352542757988\n",
      "Epoch :   897 | Loss : 0.034543536603450775\n",
      "Epoch :   898 | Loss : 0.034543536603450775\n",
      "Epoch :   899 | Loss : 0.034543536603450775\n",
      "Epoch :   901 | Loss : 0.034543536603450775\n",
      "Epoch :   902 | Loss : 0.03454352915287018\n",
      "Epoch :   903 | Loss : 0.03454354777932167\n",
      "Epoch :   904 | Loss : 0.03454354777932167\n",
      "Epoch :   905 | Loss : 0.03454354777932167\n",
      "Epoch :   906 | Loss : 0.03454354777932167\n",
      "Epoch :   907 | Loss : 0.03454354777932167\n",
      "Epoch :   908 | Loss : 0.034543540328741074\n",
      "Epoch :   909 | Loss : 0.034543540328741074\n",
      "Epoch :   911 | Loss : 0.034543536603450775\n",
      "Epoch :   912 | Loss : 0.03454352170228958\n",
      "Epoch :   913 | Loss : 0.03454352170228958\n",
      "Epoch :   914 | Loss : 0.03454352170228958\n",
      "Epoch :   915 | Loss : 0.03454352542757988\n",
      "Epoch :   916 | Loss : 0.03454352542757988\n",
      "Epoch :   917 | Loss : 0.03454352542757988\n",
      "Epoch :   918 | Loss : 0.03454352542757988\n",
      "Epoch :   919 | Loss : 0.03454352542757988\n",
      "Epoch :   921 | Loss : 0.03454352542757988\n",
      "Epoch :   922 | Loss : 0.03454352542757988\n",
      "Epoch :   923 | Loss : 0.03454352542757988\n",
      "Epoch :   924 | Loss : 0.03454352542757988\n",
      "Epoch :   925 | Loss : 0.03454352170228958\n",
      "Epoch :   926 | Loss : 0.03454352170228958\n",
      "Epoch :   927 | Loss : 0.03454352170228958\n",
      "Epoch :   928 | Loss : 0.03454352170228958\n",
      "Epoch :   929 | Loss : 0.03454352170228958\n",
      "Epoch :   931 | Loss : 0.03454352170228958\n",
      "Epoch :   932 | Loss : 0.03454352542757988\n",
      "Epoch :   933 | Loss : 0.03454351797699928\n",
      "Epoch :   934 | Loss : 0.03454351797699928\n",
      "Epoch :   935 | Loss : 0.03454351797699928\n",
      "Epoch :   936 | Loss : 0.034543514251708984\n",
      "Epoch :   937 | Loss : 0.034543514251708984\n",
      "Epoch :   938 | Loss : 0.034543514251708984\n",
      "Epoch :   939 | Loss : 0.034543514251708984\n",
      "Epoch :   941 | Loss : 0.034543510526418686\n",
      "Epoch :   942 | Loss : 0.034543510526418686\n",
      "Epoch :   943 | Loss : 0.034543510526418686\n",
      "Epoch :   944 | Loss : 0.034543510526418686\n",
      "Epoch :   945 | Loss : 0.034543514251708984\n",
      "Epoch :   946 | Loss : 0.034543510526418686\n",
      "Epoch :   947 | Loss : 0.034543510526418686\n",
      "Epoch :   948 | Loss : 0.03454352170228958\n",
      "Epoch :   949 | Loss : 0.03454351797699928\n",
      "Epoch :   951 | Loss : 0.03454352170228958\n",
      "Epoch :   952 | Loss : 0.03454352170228958\n",
      "Epoch :   953 | Loss : 0.03454352170228958\n",
      "Epoch :   954 | Loss : 0.03454352170228958\n",
      "Epoch :   955 | Loss : 0.03454351797699928\n",
      "Epoch :   956 | Loss : 0.03454351797699928\n",
      "Epoch :   957 | Loss : 0.03454351797699928\n",
      "Epoch :   958 | Loss : 0.034543514251708984\n",
      "Epoch :   959 | Loss : 0.034543514251708984\n",
      "Epoch :   961 | Loss : 0.034543510526418686\n",
      "Epoch :   962 | Loss : 0.03454349935054779\n",
      "Epoch :   963 | Loss : 0.03454349935054779\n",
      "Epoch :   964 | Loss : 0.03454350307583809\n",
      "Epoch :   965 | Loss : 0.03454350307583809\n",
      "Epoch :   966 | Loss : 0.03454350307583809\n",
      "Epoch :   967 | Loss : 0.03454350307583809\n",
      "Epoch :   968 | Loss : 0.03454350307583809\n",
      "Epoch :   969 | Loss : 0.034543491899967194\n",
      "Epoch :   971 | Loss : 0.034543491899967194\n",
      "Epoch :   972 | Loss : 0.034543491899967194\n",
      "Epoch :   973 | Loss : 0.03454350307583809\n",
      "Epoch :   974 | Loss : 0.03454350680112839\n",
      "Epoch :   975 | Loss : 0.034543510526418686\n",
      "Epoch :   976 | Loss : 0.034543510526418686\n",
      "Epoch :   977 | Loss : 0.034543510526418686\n",
      "Epoch :   978 | Loss : 0.034543510526418686\n",
      "Epoch :   979 | Loss : 0.034543510526418686\n",
      "Epoch :   981 | Loss : 0.034543488174676895\n",
      "Epoch :   982 | Loss : 0.034543488174676895\n",
      "Epoch :   983 | Loss : 0.0345434844493866\n",
      "Epoch :   984 | Loss : 0.0345434844493866\n",
      "Epoch :   985 | Loss : 0.0345434844493866\n",
      "Epoch :   986 | Loss : 0.0345434844493866\n",
      "Epoch :   987 | Loss : 0.0345434844493866\n",
      "Epoch :   988 | Loss : 0.034543491899967194\n",
      "Epoch :   989 | Loss : 0.034543491899967194\n",
      "Epoch :   991 | Loss : 0.034543488174676895\n",
      "Epoch :   992 | Loss : 0.034543488174676895\n",
      "Epoch :   993 | Loss : 0.034543488174676895\n",
      "Epoch :   994 | Loss : 0.034543491899967194\n",
      "Epoch :   995 | Loss : 0.034543488174676895\n",
      "Epoch :   996 | Loss : 0.03454349935054779\n",
      "Epoch :   997 | Loss : 0.03454349935054779\n",
      "Epoch :   998 | Loss : 0.034543491899967194\n",
      "Epoch :   999 | Loss : 0.034543491899967194\n"
     ]
    }
   ],
   "source": [
    "linear.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.034543492\n",
      "Test Loss 0.059093498\n"
     ]
    }
   ],
   "source": [
    "print('Train Loss',linear.loss_fun(ytrain,linear.predict(xtrain)).numpy())\n",
    "print('Test Loss',linear.loss_fun(ytest,linear.predict(xtest)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : 1849 Y : 3.31 Y^ : 3.309451103210449\n",
      "X : 1872 Y : 3.17 Y^ : 3.3503124713897705\n",
      "X : 1702 Y : 3.39 Y^ : 3.0482940673828125\n",
      "X : 1810 Y : 3.71 Y^ : 3.2401645183563232\n",
      "X : 1685 Y : 2.74 Y^ : 3.018092155456543\n"
     ]
    }
   ],
   "source": [
    "pred = linear.predict(xtest).numpy()\n",
    "for i in range(5):\n",
    "    print('X : {} Y : {:.2f} Y^ : {}'.format(xtest.iloc[i], ytest.iloc[i],pred[i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
